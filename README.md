# MiniGPT 🧠📝

*MiniGPT* is a minimal implementation of a GPT-style transformer-based language model built from scratch using Python and PyTorch. Inspired by OpenAI's GPT architecture, this project aims to provide a deep understanding of how transformer models work under the hood—covering everything from tokenization and self-attention to multi-head attention and text generation.

> 🚀 Ideal for students, researchers, and developers who want to learn how GPT models like ChatGPT actually work.

---

## 🔍 Features

- ✅ Character-level language model
- ✅ Bigram baseline model
- ✅ Full Transformer architecture from scratch
- ✅ Self-attention & Multi-head attention
- ✅ Positional encoding
- ✅ Feed-forward neural networks
- ✅ Text generation with sampling
- ✅ Trained on Shakespeare dataset
- ✅ No external dependencies other than PyTorch

---

## 🛠️ Tech Stack

- *Language*: Python 3.8+
- *Deep Learning Framework*: PyTorch
- *Dataset*: Shakespeare's works (character-level)
- *Development Tools*: Jupyter Notebook / VS Code
