# MiniGPT ğŸ§ ğŸ“

*MiniGPT* is a minimal implementation of a GPT-style transformer-based language model built from scratch using Python and PyTorch. Inspired by OpenAI's GPT architecture, this project aims to provide a deep understanding of how transformer models work under the hoodâ€”covering everything from tokenization and self-attention to multi-head attention and text generation.

> ğŸš€ Ideal for students, researchers, and developers who want to learn how GPT models like ChatGPT actually work.

---

## ğŸ” Features

- âœ… Character-level language model
- âœ… Bigram baseline model
- âœ… Full Transformer architecture from scratch
- âœ… Self-attention & Multi-head attention
- âœ… Positional encoding
- âœ… Feed-forward neural networks
- âœ… Text generation with sampling
- âœ… Trained on Shakespeare dataset
- âœ… No external dependencies other than PyTorch

---

## ğŸ› ï¸ Tech Stack

- *Language*: Python 3.8+
- *Deep Learning Framework*: PyTorch
- *Dataset*: Shakespeare's works (character-level)
- *Development Tools*: Jupyter Notebook / VS Code
